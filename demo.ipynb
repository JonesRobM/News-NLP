{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Headlines Topic Classifier - Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline of our PyTorch-based news headline topic classifier, from data preparation through training to inference.\n",
    "\n",
    "**Key Features:**\n",
    "- ‚ö° Fast training on sample data (~2 minutes)\n",
    "- üéØ Multiclass classification (Politics, Technology, Business, Sport)\n",
    "- üß† Neural network with embeddings and GRU\n",
    "- üìä Comprehensive evaluation metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.insert(0, '../src')\n",
    "if './src' not in sys.path:\n",
    "    sys.path.insert(0, './src')\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Project imports\n",
    "from model import NewsHeadlineClassifier\n",
    "from utils import (\n",
    "    clean_text, tokenize_text, create_vocabulary, texts_to_sequences,\n",
    "    pad_sequences, encode_labels, predict_single_headline\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset\n",
    "\n",
    "We'll create a small but representative dataset for quick demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample headlines for different topics\n",
    "sample_headlines = {\n",
    "    'politics': [\n",
    "        \"Government announces new economic stimulus package\",\n",
    "        \"Parliament debates climate change legislation today\",\n",
    "        \"Prime Minister addresses nation on healthcare reform\",\n",
    "        \"Election campaign begins with candidate announcements\",\n",
    "        \"Opposition party criticizes current tax policy\",\n",
    "        \"Senate votes on infrastructure spending bill\",\n",
    "        \"Presidential debate scheduled for next month\",\n",
    "        \"Congress approves new immigration reform measures\",\n",
    "        \"Governor signs education funding legislation\",\n",
    "        \"Political parties prepare for upcoming elections\",\n",
    "        \"Federal budget proposal sparks political debate\",\n",
    "        \"Supreme Court hears landmark constitutional case\",\n",
    "        \"Lawmakers introduce bipartisan environmental bill\",\n",
    "        \"Cabinet reshuffle announced by government officials\",\n",
    "        \"International trade agreement signed by leaders\"\n",
    "    ],\n",
    "    'technology': [\n",
    "        \"Tech giant releases revolutionary smartphone model\",\n",
    "        \"Artificial intelligence breakthrough announced by researchers\",\n",
    "        \"New social media platform gains million users\",\n",
    "        \"Cybersecurity experts warn of data breach risks\",\n",
    "        \"Quantum computing milestone achieved by scientists\",\n",
    "        \"Software company updates privacy protection measures\",\n",
    "        \"Electric vehicle manufacturer reports record sales\",\n",
    "        \"Virtual reality headset launches with advanced features\",\n",
    "        \"Cloud computing service expands global infrastructure\",\n",
    "        \"Blockchain technology adopted by major bank\",\n",
    "        \"Streaming platform introduces new content features\",\n",
    "        \"Robotics company demonstrates autonomous delivery system\",\n",
    "        \"Tech startup raises hundred million in funding\",\n",
    "        \"Gaming console breaks pre-order records worldwide\",\n",
    "        \"Internet satellite constellation completes deployment\"\n",
    "    ],\n",
    "    'business': [\n",
    "        \"Stock market reaches record high levels today\",\n",
    "        \"Major corporation reports quarterly earnings growth\",\n",
    "        \"Retail chain announces store expansion plans\",\n",
    "        \"Central bank adjusts interest rates amid inflation\",\n",
    "        \"Manufacturing sector shows signs of recovery\",\n",
    "        \"Merger creates largest company in industry\",\n",
    "        \"Cryptocurrency market experiences significant volatility\",\n",
    "        \"Oil prices surge following supply concerns\",\n",
    "        \"Real estate market shows continued growth\",\n",
    "        \"Small business confidence reaches five year high\",\n",
    "        \"International trade deficit narrows this quarter\",\n",
    "        \"Consumer spending increases during holiday season\",\n",
    "        \"Banking sector reports strong profit margins\",\n",
    "        \"Startup valued at billion dollars after funding\",\n",
    "        \"Economic indicators suggest sustained growth ahead\"\n",
    "    ],\n",
    "    'sport': [\n",
    "        \"Football team wins championship final match decisively\",\n",
    "        \"Olympic athlete breaks world record performance\",\n",
    "        \"Basketball season ends with surprising tournament results\",\n",
    "        \"Tennis player advances to semifinals competition\",\n",
    "        \"Cricket match postponed due to weather conditions\",\n",
    "        \"Swimming championship produces three new records\",\n",
    "        \"Golf tournament concludes with dramatic final round\",\n",
    "        \"Soccer world cup preparations intensify globally\",\n",
    "        \"Baseball team signs star player to contract\",\n",
    "        \"Athletics competition showcases emerging talent\",\n",
    "        \"Hockey playoffs begin with intense matchups\",\n",
    "        \"Marathon event attracts thousands of participants\",\n",
    "        \"Boxing champion defends title successfully\",\n",
    "        \"Cycling team dominates international competition\",\n",
    "        \"Winter sports season opens with spectacular events\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "data = []\n",
    "for topic, headlines in sample_headlines.items():\n",
    "    for headline in headlines:\n",
    "        data.append({'headline': headline, 'topic': topic})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"üìä Dataset Created:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Topics: {len(df['topic'].unique())}\")\n",
    "print(f\"\\nüìà Topic Distribution:\")\n",
    "print(df['topic'].value_counts())\n",
    "\n",
    "# Show sample headlines\n",
    "print(f\"\\nüì∞ Sample Headlines:\")\n",
    "for topic in df['topic'].unique():\n",
    "    sample = df[df['topic'] == topic]['headline'].iloc[0]\n",
    "    print(f\"   {topic}: \\\"{sample}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Transform raw text into numerical representations suitable for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess headlines\n",
    "print(\"üîÑ Preprocessing headlines...\")\n",
    "df['processed_headline'] = df['headline'].apply(clean_text)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab, word_to_idx, idx_to_word = create_vocabulary(\n",
    "    df['processed_headline'].tolist(),\n",
    "    min_freq=1  # Include all words for this demo\n",
    ")\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = texts_to_sequences(df['processed_headline'].tolist(), word_to_idx)\n",
    "padded_sequences, lengths = pad_sequences(sequences, max_len=30)\n",
    "\n",
    "# Encode labels\n",
    "unique_topics = sorted(df['topic'].unique())\n",
    "topic_to_idx = {topic: idx for idx, topic in enumerate(unique_topics)}\n",
    "idx_to_topic = {idx: topic for topic, idx in topic_to_idx.items()}\n",
    "labels = df['topic'].map(topic_to_idx).values\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete:\")\n",
    "print(f\"   Vocabulary size: {len(vocab)}\")\n",
    "print(f\"   Max sequence length: {padded_sequences.shape[1]}\")\n",
    "print(f\"   Number of classes: {len(unique_topics)}\")\n",
    "print(f\"   Classes: {unique_topics}\")\n",
    "\n",
    "# Show example of preprocessing\n",
    "example_idx = 0\n",
    "original = df.iloc[example_idx]['headline']\n",
    "processed = df.iloc[example_idx]['processed_headline']\n",
    "tokens = tokenize_text(processed)\n",
    "sequence = sequences[example_idx]\n",
    "\n",
    "print(f\"\\nüìù Preprocessing Example:\")\n",
    "print(f\"   Original: \\\"{original}\\\"\")\n",
    "print(f\"   Processed: \\\"{processed}\\\"\")\n",
    "print(f\"   Tokens: {tokens}\")\n",
    "print(f\"   Sequence: {sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture & Training Setup\n",
    "\n",
    "Define our neural network model and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'embedding_dim': 64,  # Smaller for demo\n",
    "    'hidden_dim': 32,     # Smaller for demo\n",
    "    'num_classes': len(unique_topics),\n",
    "    'use_gru': True,\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 16,     # Smaller for demo\n",
    "    'num_epochs': 15,     # Fewer epochs for demo\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "print(f\"üß† Model Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "model = NewsHeadlineClassifier(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    embedding_dim=config['embedding_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    num_classes=config['num_classes'],\n",
    "    use_gru=config['use_gru'],\n",
    "    dropout_rate=config['dropout_rate']\n",
    ").to(config['device'])\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüî¢ Model Parameters:\")\n",
    "print(f\"   Total: {total_params:,}\")\n",
    "print(f\"   Trainable: {trainable_params:,}\")\n",
    "\n",
    "# Model architecture summary\n",
    "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
    "print(f\"   Input ‚Üí Embedding({config['vocab_size']}, {config['embedding_dim']})\")\n",
    "if config['use_gru']:\n",
    "    print(f\"   ‚Üí Bidirectional GRU({config['embedding_dim']}, {config['hidden_dim']})\")\n",
    "    print(f\"   ‚Üí Dropout({config['dropout_rate']})\")\n",
    "    print(f\"   ‚Üí Linear({config['hidden_dim']*2}, {config['num_classes']}) ‚Üí Output\")\n",
    "else:\n",
    "    print(f\"   ‚Üí Mean Pooling\")\n",
    "    print(f\"   ‚Üí Dropout({config['dropout_rate']})\")\n",
    "    print(f\"   ‚Üí Linear({config['embedding_dim']}, {config['num_classes']}) ‚Üí Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "sequences_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(sequences_tensor, lengths_tensor, labels_tensor)\n",
    "\n",
    "# Split into train and validation (80/20)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Train the model with real-time progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(f\"üöÄ Starting training for {config['num_epochs']} epochs...\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config['num_epochs']):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (input_ids, lengths, labels) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(config['device'])\n",
    "        lengths = lengths.to(config['device'])\n",
    "        labels = labels.to(config['device'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, lengths, labels in val_loader:\n",
    "            input_ids = input_ids.to(config['device'])\n",
    "            lengths = lengths.to(config['device'])\n",
    "            labels = labels.to(config['device'])\n",
    "            \n",
    "            outputs = model(input_ids, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    # Store history\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1:2d}/{config['num_epochs']} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Final validation accuracy: {val_accuracies[-1]:.4f}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization\n",
    "\n",
    "Visualize the training progress and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(epochs, val_accuracies, 'g-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training summary\n",
    "best_epoch = np.argmax(val_accuracies) + 1\n",
    "print(f\"üìà Training Summary:\")\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accuracies):.4f}\")\n",
    "print(f\"   Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"   Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation with confusion matrix and classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, lengths, labels in val_loader:\n",
    "        input_ids = input_ids.to(config['device'])\n",
    "        lengths = lengths.to(config['device'])\n",
    "        \n",
    "        outputs = model(input_ids, lengths)\n",
    "        probabilities = torch.softmax(outputs, dim=-1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "report = classification_report(\n",
    "    all_labels, all_predictions, \n",
    "    target_names=unique_topics, \n",
    "    digits=3\n",
    ")\n",
    "\n",
    "print(f\"üìä Model Evaluation Results:\")\n",
    "print(f\"   Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=unique_topics,\n",
    "    yticklabels=unique_topics,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "print(f\"\\nüéØ Per-Class Accuracy:\")\n",
    "for i, topic in enumerate(unique_topics):\n",
    "    print(f\"   {topic}: {class_accuracies[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Inference Demo\n",
    "\n",
    "Test the trained model with new headlines and see real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_predict(headline, show_probabilities=True):\n",
    "    \"\"\"Predict topic for a single headline with detailed output.\"\"\"\n",
    "    \n",
    "    # Preprocess\n",
    "    cleaned = clean_text(headline)\n",
    "    tokens = tokenize_text(cleaned)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    unk_idx = word_to_idx.get('<unk>', 1)\n",
    "    sequence = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
    "    \n",
    "    # Pad sequence\n",
    "    max_len = 30\n",
    "    if len(sequence) >= max_len:\n",
    "        padded_sequence = sequence[:max_len]\n",
    "        length = max_len\n",
    "    else:\n",
    "        padded_sequence = sequence + [0] * (max_len - len(sequence))\n",
    "        length = len(sequence)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor([padded_sequence], dtype=torch.long).to(config['device'])\n",
    "        lengths = torch.tensor([length], dtype=torch.long).to(config['device'])\n",
    "        \n",
    "        outputs = model(input_ids, lengths)\n",
    "        probabilities = torch.softmax(outputs, dim=-1).cpu().numpy()[0]\n",
    "        predicted_idx = np.argmax(probabilities)\n",
    "    \n",
    "    predicted_topic = unique_topics[predicted_idx]\n",
    "    confidence = probabilities[predicted_idx]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüì∞ Headline: \\\"{headline}\\\"\")\n",
    "    print(f\"üéØ Predicted Topic: {predicted_topic}\")\n",
    "    print(f\"üìä Confidence: {confidence:.3f}\")\n",
    "    \n",
    "    if show_probabilities:\n",
    "        print(f\"\\nüèÜ All Topic Probabilities:\")\n",
    "        sorted_indices = np.argsort(probabilities)[::-1]\n",
    "        for i, idx in enumerate(sorted_indices):\n",
    "            topic = unique_topics[idx]\n",
    "            prob = probabilities[idx]\n",
    "            bar_length = int(prob * 20)\n",
    "            bar = \"‚ñà\" * bar_length + \"‚ñë\" * (20 - bar_length)\n",
    "            print(f\"   {i+1}. {topic:12} {bar} {prob:.3f}\")\n",
    "    \n",
    "    return predicted_topic, confidence\n",
    "\n",
    "# Test with example headlines\n",
    "test_headlines = [\n",
    "    \"Apple announces new iPhone with advanced AI features\",\n",
    "    \"President signs new climate change legislation\",\n",
    "    \"Stock market surges following positive economic data\",\n",
    "    \"Tennis champion wins Wimbledon for third consecutive year\",\n",
    "    \"Scientists develop breakthrough quantum computing processor\"\n",
    "]\n",
    "\n",
    "print(\"üîÆ Testing Model with New Headlines:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, headline in enumerate(test_headlines, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test {i}/5\")\n",
    "    demo_predict(headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Analysis\n",
    "\n",
    "Analyze what the model has learned and identify potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution analysis\n",
    "confidences = [max(probs) for probs in all_probabilities]\n",
    "correct_mask = [pred == true for pred, true in zip(all_predictions, all_labels)]\n",
    "\n",
    "correct_confidences = [conf for conf, correct in zip(confidences, correct_mask) if correct]\n",
    "incorrect_confidences = [conf for conf, correct in zip(confidences, correct_mask) if not correct]\n",
    "\n",
    "# Plot confidence distributions\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(correct_confidences, bins=10, alpha=0.7, label='Correct', color='green')\n",
    "plt.hist(incorrect_confidences, bins=10, alpha=0.7, label='Incorrect', color='red')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Confidence Distribution by Correctness')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(confidences, bins=15, alpha=0.7, color='blue')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Overall Confidence Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üîç Confidence Analysis:\")\n",
    "print(f\"   Average confidence (correct): {np.mean(correct_confidences):.3f}\")\n",
    "print(f\"   Average confidence (incorrect): {np.mean(incorrect_confidences):.3f}\")\n",
    "print(f\"   Overall average confidence: {np.mean(confidences):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most confident correct and incorrect predictions\n",
    "val_headlines = [df.iloc[i]['headline'] for i in val_dataset.indices]\n",
    "val_topics = [df.iloc[i]['topic'] for i in val_dataset.indices]\n",
    "\n",
    "# Combine results\n",
    "results = []\n",
    "for i, (headline, true_topic, pred_idx, true_idx, conf) in enumerate(\n",
    "    zip(val_headlines, val_topics, all_predictions, all_labels, confidences)\n",
    "):\n",
    "    results.append({\n",
    "        'headline': headline,\n",
    "        'true_topic': true_topic,\n",
    "        'predicted_topic': unique_topics[pred_idx],\n",
    "        'confidence': conf,\n",
    "        'correct': pred_idx == true_idx\n",
    "    })\n",
    "\n",
    "# Sort by confidence\n",
    "results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "print(f\"\\nüéØ Most Confident Correct Predictions:\")\n",
    "correct_results = [r for r in results if r['correct']]\n",
    "for i, result in enumerate(correct_results[:3], 1):\n",
    "    print(f\"   {i}. \\\"{result['headline'][:60]}...\\\"\")\n",
    "    print(f\"      Topic: {result['predicted_topic']} (confidence: {result['confidence']:.3f})\\n\")\n",
    "\n",
    "print(f\"‚ùå Most Confident Incorrect Predictions:\")\n",
    "incorrect_results = [r for r in results if not r['correct']]\n",
    "if incorrect_results:\n",
    "    for i, result in enumerate(incorrect_results[:2], 1):\n",
    "        print(f\"   {i}. \\\"{result['headline'][:60]}...\\\"\")\n",
    "        print(f\"      Predicted: {result['predicted_topic']} | True: {result['true_topic']}\")\n",
    "        print(f\"      Confidence: {result['confidence']:.3f}\\n\")\n",
    "else:\n",
    "    print(f\"   üéâ No incorrect predictions! Perfect model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Next Steps\n",
    "\n",
    "Wrap up with key insights and recommendations for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ NEWS HEADLINES TOPIC CLASSIFIER - DEMO COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Final Performance Summary:\")\n",
    "print(f\"   üìà Validation Accuracy: {accuracy:.1%}\")\n",
    "print(f\"   üéØ Number of Classes: {len(unique_topics)}\")\n",
    "print(f\"   üìö Vocabulary Size: {len(vocab):,} words\")\n",
    "print(f\"   üî¢ Model Parameters: {total_params:,}\")\n",
    "print(f\"   ‚ö° Training Time: ~{config['num_epochs']} epochs\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Performing Topics:\")\n",
    "for i, topic in enumerate(unique_topics):\n",
    "    acc = class_accuracies[i]\n",
    "    status = \"ÔøΩÔøΩ\" if acc > 0.9 else \"ü•à\" if acc > 0.8 else \"ÔøΩÔøΩ\" if acc > 0.7 else \"üìà\"\n",
    "    print(f\"   {status} {topic}: {acc:.1%}\")\n",
    "\n",
    "print(f\"\\nüîÆ Model Capabilities Demonstrated:\")\n",
    "print(f\"   ‚úÖ Text preprocessing and tokenization\")\n",
    "print(f\"   ‚úÖ Neural embedding layer for word representations\")\n",
    "print(f\"   ‚úÖ Bidirectional GRU for sequence modeling\")\n",
    "print(f\"   ‚úÖ Multiclass classification with softmax\")\n",
    "print(f\"   ‚úÖ Training with early stopping and regularization\")\n",
    "print(f\"   ‚úÖ Comprehensive evaluation and visualization\")\n",
    "print(f\"   ‚úÖ Real-time inference on new headlines\")\n",
    "\n",
    "print(f\"\\nüöÄ Potential Improvements:\")\n",
    "print(f\"   üìä Collect more diverse training data\")\n",
    "print(f\"   üß† Experiment with transformer models (BERT, RoBERTa)\")\n",
    "print(f\"   üéØ Add more topic categories\")\n",
    "print(f\"   ‚öñÔ∏è Implement class balancing techniques\")\n",
    "print(f\"   üîß Hyperparameter optimization\")\n",
    "print(f\"   üåê Deploy as web service or API\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"   1. Save this trained model for future use\")\n",
    "print(f\"   2. Test with real RSS feed data\")\n",
    "print(f\"   3. Experiment with different architectures\")\n",
    "print(f\"   4. Build a web interface for easy interaction\")\n",
    "print(f\"   5. Scale up with larger datasets\")\n",
    "\n",
    "print(f\"\\nüéì Skills Demonstrated:\")\n",
    "print(f\"   ‚Ä¢ PyTorch neural network implementation\")\n",
    "print(f\"   ‚Ä¢ NLP preprocessing and tokenization\")\n",
    "print(f\"   ‚Ä¢ Training loop with validation\")\n",
    "print(f\"   ‚Ä¢ Model evaluation and metrics\")\n",
    "print(f\"   ‚Ä¢ Data visualization with matplotlib/seaborn\")\n",
    "print(f\"   ‚Ä¢ Clean, documented code following best practices\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚ú® Thank you for exploring the News Headlines Topic Classifier! ‚ú®\")\n",
    "print(f\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
