{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Headlines Data Collection\n",
    "\n",
    "This notebook demonstrates how to collect news headlines from RSS feeds for the topic classification project.\n",
    "\n",
    "**Important**: Always respect website terms of service and implement appropriate delays between requests to avoid overloading servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urljoin\n",
    "import feedparser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project src to path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils import clean_text, setup_logging\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS Feed URLs\n",
    "\n",
    "Define RSS feed URLs for different news sources and topics. These are publicly available RSS feeds that can be used for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS feed URLs organized by source and topic\n",
    "RSS_FEEDS = {\n",
    "    'bbc': {\n",
    "        'politics': 'http://feeds.bbci.co.uk/news/politics/rss.xml',\n",
    "        'technology': 'http://feeds.bbci.co.uk/news/technology/rss.xml',\n",
    "        'business': 'http://feeds.bbci.co.uk/news/business/rss.xml',\n",
    "        'sport': 'http://feeds.bbci.co.uk/sport/rss.xml',\n",
    "        'health': 'http://feeds.bbci.co.uk/news/health/rss.xml'\n",
    "    },\n",
    "    'reuters': {\n",
    "        'politics': 'https://feeds.reuters.com/reuters/politicsNews',\n",
    "        'technology': 'https://feeds.reuters.com/reuters/technologyNews',\n",
    "        'business': 'https://feeds.reuters.com/reuters/businessNews',\n",
    "        'sport': 'https://feeds.reuters.com/reuters/sportsNews',\n",
    "        'health': 'https://feeds.reuters.com/reuters/healthNews'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display available feeds\n",
    "print(\"Available RSS Feeds:\")\n",
    "for source, topics in RSS_FEEDS.items():\n",
    "    print(f\"\\n{source.upper()}:\")\n",
    "    for topic, url in topics.items():\n",
    "        print(f\"  {topic}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rss_feed(url: str, timeout: int = 10) -> Optional[feedparser.FeedParserDict]:\n",
    "    \"\"\"\n",
    "    Fetch and parse an RSS feed.\n",
    "    \n",
    "    Args:\n",
    "        url (str): RSS feed URL.\n",
    "        timeout (int): Request timeout in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[feedparser.FeedParserDict]: Parsed feed or None if failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set user agent to be respectful\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; NewsClassifier/1.0; Educational Use)'\n",
    "        }\n",
    "        \n",
    "        # Parse feed\n",
    "        feed = feedparser.parse(url)\n",
    "        \n",
    "        if feed.bozo:\n",
    "            logger.warning(f\"Warning: Feed parsing issues for {url}: {feed.bozo_exception}\")\n",
    "        \n",
    "        return feed\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching feed {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_headlines_from_feed(\n",
    "    feed: feedparser.FeedParserDict, \n",
    "    topic: str, \n",
    "    source: str,\n",
    "    max_items: int = None\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract headlines from a parsed RSS feed.\n",
    "    \n",
    "    Args:\n",
    "        feed (feedparser.FeedParserDict): Parsed RSS feed.\n",
    "        topic (str): Topic category.\n",
    "        source (str): News source name.\n",
    "        max_items (int, optional): Maximum number of items to extract.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: List of headline data.\n",
    "    \"\"\"\n",
    "    headlines = []\n",
    "    \n",
    "    items = feed.entries[:max_items] if max_items else feed.entries\n",
    "    \n",
    "    for entry in items:\n",
    "        # Extract headline (title)\n",
    "        headline = entry.get('title', '').strip()\n",
    "        \n",
    "        if not headline:\n",
    "            continue\n",
    "        \n",
    "        # Extract additional information\n",
    "        description = entry.get('summary', '').strip()\n",
    "        link = entry.get('link', '')\n",
    "        published = entry.get('published', '')\n",
    "        \n",
    "        # Clean headline\n",
    "        cleaned_headline = clean_text(headline)\n",
    "        \n",
    "        if len(cleaned_headline) > 5:  # Filter out very short headlines\n",
    "            headlines.append({\n",
    "                'headline': headline,\n",
    "                'cleaned_headline': cleaned_headline,\n",
    "                'topic': topic,\n",
    "                'source': source,\n",
    "                'description': description,\n",
    "                'link': link,\n",
    "                'published': published,\n",
    "                'collected_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    return headlines\n",
    "\n",
    "\n",
    "def collect_headlines_from_feeds(\n",
    "    rss_feeds: Dict[str, Dict[str, str]],\n",
    "    max_per_topic: int = 100,\n",
    "    delay_between_requests: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect headlines from multiple RSS feeds.\n",
    "    \n",
    "    Args:\n",
    "        rss_feeds (Dict[str, Dict[str, str]]): Dictionary of RSS feed URLs.\n",
    "        max_per_topic (int): Maximum headlines per topic.\n",
    "        delay_between_requests (float): Delay between requests in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing collected headlines.\n",
    "    \"\"\"\n",
    "    all_headlines = []\n",
    "    \n",
    "    for source, topics in rss_feeds.items():\n",
    "        logger.info(f\"Collecting headlines from {source.upper()}\")\n",
    "        \n",
    "        for topic, url in topics.items():\n",
    "            logger.info(f\"  Fetching {topic} headlines...\")\n",
    "            \n",
    "            # Fetch feed\n",
    "            feed = fetch_rss_feed(url)\n",
    "            \n",
    "            if feed is None:\n",
    "                logger.warning(f\"  Failed to fetch {topic} from {source}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract headlines\n",
    "            headlines = extract_headlines_from_feed(\n",
    "                feed, topic, source, max_items=max_per_topic\n",
    "            )\n",
    "            \n",
    "            all_headlines.extend(headlines)\n",
    "            logger.info(f\"  Collected {len(headlines)} {topic} headlines from {source}\")\n",
    "            \n",
    "            # Be respectful - add delay between requests\n",
    "            time.sleep(delay_between_requests)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_headlines)\n",
    "    \n",
    "    logger.info(f\"Total headlines collected: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "print(\"Functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RSS Feed Access\n",
    "\n",
    "Let's test accessing one RSS feed to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with BBC Technology feed\n",
    "test_url = RSS_FEEDS['bbc']['technology']\n",
    "print(f\"Testing RSS feed: {test_url}\")\n",
    "\n",
    "test_feed = fetch_rss_feed(test_url)\n",
    "\n",
    "if test_feed:\n",
    "    print(f\"\\nFeed title: {test_feed.feed.get('title', 'N/A')}\")\n",
    "    print(f\"Feed description: {test_feed.feed.get('description', 'N/A')}\")\n",
    "    print(f\"Number of entries: {len(test_feed.entries)}\")\n",
    "    \n",
    "    # Show first few headlines\n",
    "    print(\"\\nSample headlines:\")\n",
    "    for i, entry in enumerate(test_feed.entries[:5]):\n",
    "        print(f\"{i+1}. {entry.get('title', 'No title')}\")\n",
    "else:\n",
    "    print(\"Failed to fetch test feed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Full Dataset\n",
    "\n",
    "Now let's collect headlines from all configured RSS feeds. \n",
    "\n",
    "**Note**: This will make multiple requests to news websites. Please be respectful:\n",
    "- We include delays between requests\n",
    "- We use appropriate user agent strings\n",
    "- We only collect what we need for educational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data collection\n",
    "CONFIG = {\n",
    "    'max_headlines_per_topic': 150,  # Collect a bit more than needed\n",
    "    'delay_between_requests': 2.0,   # 2 second delay between requests\n",
    "    'target_topics': ['politics', 'technology', 'business', 'sport']  # Select 4 topics\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nStarting data collection...\")\n",
    "print(f\"This will take approximately {len(RSS_FEEDS) * len(CONFIG['target_topics']) * CONFIG['delay_between_requests'] / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter RSS feeds to only include target topics\n",
    "filtered_feeds = {}\n",
    "for source, topics in RSS_FEEDS.items():\n",
    "    filtered_feeds[source] = {}\n",
    "    for topic in CONFIG['target_topics']:\n",
    "        if topic in topics:\n",
    "            filtered_feeds[source][topic] = topics[topic]\n",
    "\n",
    "print(\"Filtered RSS feeds:\")\n",
    "for source, topics in filtered_feeds.items():\n",
    "    print(f\"\\n{source.upper()}:\")\n",
    "    for topic, url in topics.items():\n",
    "        print(f\"  {topic}: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "start_time = time.time()\n",
    "\n",
    "df_collected = collect_headlines_from_feeds(\n",
    "    rss_feeds=filtered_feeds,\n",
    "    max_per_topic=CONFIG['max_headlines_per_topic'],\n",
    "    delay_between_requests=CONFIG['delay_between_requests']\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\nData collection completed in {duration:.1f} seconds\")\n",
    "print(f\"Total headlines collected: {len(df_collected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the collected data\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total samples: {len(df_collected)}\")\n",
    "print(f\"Columns: {list(df_collected.columns)}\")\n",
    "\n",
    "print(\"\\nSamples per topic:\")\n",
    "topic_counts = df_collected['topic'].value_counts()\n",
    "print(topic_counts)\n",
    "\n",
    "print(\"\\nSamples per source:\")\n",
    "source_counts = df_collected['source'].value_counts()\n",
    "print(source_counts)\n",
    "\n",
    "print(\"\\nTopic-Source distribution:\")\n",
    "crosstab = pd.crosstab(df_collected['topic'], df_collected['source'])\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample headlines for each topic\n",
    "print(\"Sample headlines by topic:\")\n",
    "for topic in df_collected['topic'].unique():\n",
    "    print(f\"\\n{topic.upper()}:\")\n",
    "    sample_headlines = df_collected[df_collected['topic'] == topic]['headline'].head(3)\n",
    "    for i, headline in enumerate(sample_headlines, 1):\n",
    "        print(f\"  {i}. {headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the dataset\n",
    "print(\"Cleaning dataset...\")\n",
    "\n",
    "# Remove duplicates based on cleaned headline\n",
    "initial_count = len(df_collected)\n",
    "df_cleaned = df_collected.drop_duplicates(subset=['cleaned_headline'], keep='first')\n",
    "print(f\"Removed {initial_count - len(df_cleaned)} duplicate headlines\")\n",
    "\n",
    "# Remove very short or very long headlines\n",
    "headline_lengths = df_cleaned['cleaned_headline'].str.split().str.len()\n",
    "df_cleaned = df_cleaned[(headline_lengths >= 3) & (headline_lengths <= 50)]\n",
    "print(f\"Removed headlines outside 3-50 word range. Remaining: {len(df_cleaned)}\")\n",
    "\n",
    "# Balance the dataset - take equal samples from each topic\n",
    "min_samples = df_cleaned['topic'].value_counts().min()\n",
    "target_samples_per_topic = min(min_samples, 500)  # Max 500 per topic\n",
    "\n",
    "print(f\"\\nBalancing dataset to {target_samples_per_topic} samples per topic...\")\n",
    "\n",
    "balanced_dfs = []\n",
    "for topic in df_cleaned['topic'].unique():\n",
    "    topic_df = df_cleaned[df_cleaned['topic'] == topic].sample(\n",
    "        n=target_samples_per_topic, \n",
    "        random_state=42\n",
    "    )\n",
    "    balanced_dfs.append(topic_df)\n",
    "\n",
    "df_final = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Final dataset size: {len(df_final)}\")\n",
    "print(\"\\nFinal topic distribution:\")\n",
    "print(df_final['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save raw collected data\n",
    "raw_file = '../data/raw/headlines_raw.csv'\n",
    "df_collected.to_csv(raw_file, index=False)\n",
    "print(f\"Raw data saved to: {raw_file}\")\n",
    "\n",
    "# Save processed data for training\n",
    "processed_file = '../data/processed/headlines.csv'\n",
    "# Select only required columns for training\n",
    "df_training = df_final[['headline', 'topic']].copy()\n",
    "df_training.to_csv(processed_file, index=False)\n",
    "print(f\"Processed data saved to: {processed_file}\")\n",
    "\n",
    "# Save full processed data with metadata\n",
    "full_processed_file = '../data/processed/headlines_full.csv'\n",
    "df_final.to_csv(full_processed_file, index=False)\n",
    "print(f\"Full processed data saved to: {full_processed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display dataset statistics\n",
    "print(\"Final Dataset Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Total samples: {len(df_training)}\")\n",
    "print(f\"Number of topics: {df_training['topic'].nunique()}\")\n",
    "print(f\"Topics: {sorted(df_training['topic'].unique())}\")\n",
    "\n",
    "# Headline length statistics\n",
    "headline_words = df_training['headline'].str.split().str.len()\n",
    "print(f\"\\nHeadline length statistics:\")\n",
    "print(f\"  Mean: {headline_words.mean():.1f} words\")\n",
    "print(f\"  Median: {headline_words.median():.1f} words\")\n",
    "print(f\"  Min: {headline_words.min()} words\")\n",
    "print(f\"  Max: {headline_words.max()} words\")\n",
    "\n",
    "# Vocabulary size estimate\n",
    "all_words = ' '.join(df_training['headline']).lower().split()\n",
    "unique_words = set(all_words)\n",
    "print(f\"\\nEstimated vocabulary size: {len(unique_words)} unique words\")\n",
    "\n",
    "print(\"\\nDataset is ready for training!\")\n",
    "print(f\"You can now run the training script with: python src/train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Create Sample Data\n",
    "\n",
    "If you cannot access RSS feeds, you can create sample data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you want to create sample data instead\n",
    "\n",
    "# from utils import create_sample_data\n",
    "\n",
    "# print(\"Creating sample data for testing...\")\n",
    "# create_sample_data(\n",
    "#     output_path='../data/processed/headlines.csv',\n",
    "#     num_samples_per_topic=200\n",
    "# )\n",
    "# print(\"Sample data created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
